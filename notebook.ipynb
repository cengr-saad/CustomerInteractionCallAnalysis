{"cells":[{"source":["A retail company is on a transformative journey, aiming to elevate their customer services through cutting-edge advancements in Speech Recognition and Natural Language Processing (NLP). As the machine learning engineer for this initiative, you are tasked with developing functionalities that not only convert customer support audio calls into text but also explore methodologies to extract insights from transcribed texts.\n","\n","In this dynamic project, we leverage the power of `SpeechRecognition`, `Pydub`, and `spaCy` – three open-source packages that form the backbone of your solution. Your objectives are:\n","  - Transcribe a sample customer audio call, stored at `sample_customer_call.wav`, to showcase the power of open-source speech recognition technology.\n","  - Analyze sentiment, identify common named entities, and enhance user experience by searching for the most similar customer calls based on a given query from a subset of their pre-transcribed call data, stored at `customer_call_transcriptions.csv`.\n","\n","This project is an opportunity to unlock the potential of machine learning to revolutionize customer support. Let's delve into the interplay between technology and service excellence."],"metadata":{"id":"d5e81b43-ccfd-4fc6-902c-59cd49aa9913"},"id":"d5e81b43-ccfd-4fc6-902c-59cd49aa9913","cell_type":"markdown"},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FzlGCJqvsWk9","executionInfo":{"status":"ok","timestamp":1723964721405,"user_tz":-300,"elapsed":35665,"user":{"displayName":"Saad R","userId":"07513358746274001782"}},"outputId":"33b6f25a-7ec2-47c9-956a-bcef4cd44c74"},"id":"FzlGCJqvsWk9","execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"source":["!pip install SpeechRecognition\n","!pip install pydub\n","!pip install spacy\n","!python3 -m spacy download en_core_web_sm"],"metadata":{"executionCancelledAt":null,"executionTime":29960,"lastExecutedAt":1719850530360,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"!pip install SpeechRecognition\n!pip install pydub\n!pip install spacy\n!python3 -m spacy download en_core_web_sm","outputsMetadata":{"0":{"height":613,"type":"stream"}},"collapsed":true,"jupyter":{"outputs_hidden":true,"source_hidden":false},"lastExecutedByKernel":"a92e50f9-461c-4d7f-8096-a48d24039401","colab":{"base_uri":"https://localhost:8080/"},"id":"d0f1598e-18a8-45d5-8387-bf2f5ce4ffd6","executionInfo":{"status":"ok","timestamp":1723964751615,"user_tz":-300,"elapsed":30220,"user":{"displayName":"Saad R","userId":"07513358746274001782"}},"outputId":"cb6a54a4-7f80-4c37-9afa-009e663a6652"},"id":"d0f1598e-18a8-45d5-8387-bf2f5ce4ffd6","cell_type":"code","execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting SpeechRecognition\n","  Downloading SpeechRecognition-3.10.4-py2.py3-none-any.whl.metadata (28 kB)\n","Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from SpeechRecognition) (2.32.3)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from SpeechRecognition) (4.12.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->SpeechRecognition) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->SpeechRecognition) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->SpeechRecognition) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->SpeechRecognition) (2024.7.4)\n","Downloading SpeechRecognition-3.10.4-py2.py3-none-any.whl (32.8 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m32.8/32.8 MB\u001b[0m \u001b[31m37.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: SpeechRecognition\n","Successfully installed SpeechRecognition-3.10.4\n","Collecting pydub\n","  Downloading pydub-0.25.1-py2.py3-none-any.whl.metadata (1.4 kB)\n","Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n","Installing collected packages: pydub\n","Successfully installed pydub-0.25.1\n","Requirement already satisfied: spacy in /usr/local/lib/python3.10/dist-packages (3.7.5)\n","Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.12)\n","Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.5)\n","Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.10)\n","Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.8)\n","Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.9)\n","Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (8.2.5)\n","Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.1.3)\n","Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.4.8)\n","Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.10)\n","Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.4.1)\n","Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.12.3)\n","Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (4.66.5)\n","Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.32.3)\n","Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.8.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.1.4)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy) (71.0.4)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (24.1)\n","Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.4.0)\n","Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.26.4)\n","Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.2.0)\n","Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.20.1)\n","Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.12.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2024.7.4)\n","Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.7.11)\n","Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.1.5)\n","Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (8.1.7)\n","Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n","Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (13.7.1)\n","Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.18.1)\n","Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.0.4)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy) (2.1.5)\n","Requirement already satisfied: marisa-trie>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.0)\n","Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n","Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.16.1)\n","Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.16.0)\n","Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n","Collecting en-core-web-sm==3.7.1\n","  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.1/en_core_web_sm-3.7.1-py3-none-any.whl (12.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m67.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.2 in /usr/local/lib/python3.10/dist-packages (from en-core-web-sm==3.7.1) (3.7.5)\n","Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.12)\n","Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.5)\n","Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.10)\n","Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.8)\n","Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.9)\n","Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.2.5)\n","Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.1.3)\n","Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.4.8)\n","Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.10)\n","Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.4.1)\n","Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.12.3)\n","Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.66.5)\n","Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.32.3)\n","Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.8.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.1.4)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (71.0.4)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (24.1)\n","Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.4.0)\n","Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.26.4)\n","Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.2.0)\n","Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.20.1)\n","Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.12.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2024.7.4)\n","Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.11)\n","Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.5)\n","Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.1.7)\n","Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.5.4)\n","Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (13.7.1)\n","Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.18.1)\n","Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (7.0.4)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.1.5)\n","Requirement already satisfied: marisa-trie>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.2.0)\n","Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.0)\n","Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.16.1)\n","Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.16.0)\n","Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.2)\n","\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n","You can now load the package via spacy.load('en_core_web_sm')\n","\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n","If you are in a Jupyter or Colab notebook, you may need to restart Python in\n","order to load all the package's dependencies. You can do this by selecting the\n","'Restart kernel' or 'Restart runtime' option.\n"]}]},{"source":["# Import required libraries\n","import pandas as pd\n","\n","import nltk\n","nltk.download('vader_lexicon')\n","\n","from nltk.sentiment.vader import SentimentIntensityAnalyzer\n","\n","import speech_recognition as sr\n","from pydub import AudioSegment\n","\n","import spacy"],"metadata":{"executionCancelledAt":null,"executionTime":50,"lastExecutedAt":1719850530412,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Import required libraries\nimport pandas as pd\n\nimport nltk\nnltk.download('vader_lexicon')\n\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer\n\nimport speech_recognition as sr\nfrom pydub import AudioSegment\n\nimport spacy","outputsMetadata":{"0":{"height":77,"type":"stream"}},"collapsed":true,"jupyter":{"outputs_hidden":true,"source_hidden":false},"lastExecutedByKernel":"a92e50f9-461c-4d7f-8096-a48d24039401","colab":{"base_uri":"https://localhost:8080/"},"id":"d6f3dd61-8c75-48d4-b2a5-79cd0b444ddb","executionInfo":{"status":"ok","timestamp":1723964766972,"user_tz":-300,"elapsed":13534,"user":{"displayName":"Saad R","userId":"07513358746274001782"}},"outputId":"e7fc9289-ed57-488c-d734-c951ebe21008"},"id":"d6f3dd61-8c75-48d4-b2a5-79cd0b444ddb","cell_type":"code","execution_count":3,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n"]}]},{"source":["# Task 1 - Speech to Text: convert the sample audio call, sample_customer_call.wav, to text and store the result in transcribed_text\n","\n","# Define a recognizer object\n","recognizer = sr.Recognizer()\n","\n","# Convert the audio file to audio data\n","transcribe_audio_file = sr.AudioFile(\"/content/drive/MyDrive/Projects/Data Science/Customer Support Call Analysis/sample_customer_call.wav\")\n","with transcribe_audio_file as source:\n","    transcribe_audio = recognizer.record(source)\n","\n","# Convert the audio data to text\n","transcribed_text = recognizer.recognize_google(transcribe_audio)\n","\n","# Review trascribed text\n","print(\"Transcribed text: \", transcribed_text)\n","\n","# Task 1 - Speech to Text: store few statistics of the audio file such as number of channels, sample width and frame rate\n","\n","# Review number of channels and frame rate of the audio file\n","audio_segment = AudioSegment.from_file(\"/content/drive/MyDrive/Projects/Data Science/Customer Support Call Analysis/sample_customer_call.wav\")\n","number_channels = audio_segment.channels\n","frame_rate = audio_segment.frame_rate\n","\n","print(\"Number of channels: \", number_channels)\n","print(\"Frequency: \", frame_rate)\n","\n","# Task 2 - Sentiment Analysis: use vader module from nltk library to determine the sentiment of each text of the customer_call_transcriptions.csv file and store them at a new sentiment_label column using compound score\n","\n","# Import customer call transcriptions data\n","df = pd.read_csv(\"/content/drive/MyDrive/Projects/Data Science/Customer Support Call Analysis/customer_call_transcriptions.csv\")\n","\n","sid = SentimentIntensityAnalyzer()\n","\n","# Analyze sentiment by evaluating compound score generated by Vader SentimentIntensityAnalyzer\n","def find_sentiment(text):\n","    scores = sid.polarity_scores(text)\n","    compound_score = scores['compound']\n","\n","    if compound_score >= 0.05:\n","        return 'positive'\n","    elif compound_score <= -0.05:\n","        return 'negative'\n","    else:\n","        return 'neutral'\n","\n","df['sentiment_predicted'] = df.apply(lambda row: find_sentiment(row[\"text\"]), axis = 1)\n","\n","# Task 2 - Sentiment Analysis: calculate number of texts with positive label that are correctly labeled as positive\n","true_positive = len(df.loc[(df['sentiment_predicted'] == df['sentiment_label']) &\n","                (df['sentiment_label'] == 'positive')])\n","\n","print(\"True positives: \", true_positive)\n","\n","# Task 3 - Named Entity Recognition: find named entities for each text in the df object and store entities in a named_entities column\n","\n","# Load spaCy small English Language model\n","nlp = spacy.load(\"en_core_web_sm\")\n","\n","# NER using spaCy\n","def extract_entities(text):\n","    doc = nlp(text)\n","    entities = [ent.text for ent in doc.ents]\n","    return entities\n","\n","# Apply NER to the entire text column\n","df['named_entities'] = df['text'].apply(extract_entities)\n","\n","# Flatten the list of named entities\n","all_entities = [ent for entities in df['named_entities'] for ent in entities]\n","\n","# Create a DataFrame with the counts\n","entities_df = pd.DataFrame(all_entities, columns=['entity'])\n","entities_counts = entities_df['entity'].value_counts().reset_index()\n","entities_counts.columns = ['entity', 'count']\n","\n","# Extract most frequent named entity\n","most_freq_ent = entities_counts[\"entity\"].iloc[0]\n","print(\"Most frequent entity: \", most_freq_ent)\n","\n","# Task 4 - Find most similar text: find the list of customer calls that complained about \"wrong package delivery\" by finding similarity score of each text to the \"wrong package delivery\" string using spaCy small English Language model\n","\n","# Load spaCy model\n","nlp = spacy.load(\"en_core_web_sm\")\n","\n","# Process the text column\n","df['processed_text'] = df['text'].apply(lambda text: nlp(text))\n","\n","# Input query\n","input_query = \"wrong package delivery\"\n","processed_query = nlp(input_query)\n","\n","# Calculate similarity scores and sort dataframe with respect to similarity scores\n","df['similarity'] = df['processed_text'].apply(lambda text: processed_query.similarity(text))\n","df = df.sort_values(by='similarity', ascending=False)\n","\n","# Find the most similar text\n","most_similar_text = df[\"text\"].iloc[0]\n","print(\"Most similar text: \", most_similar_text)"],"metadata":{"executionCancelledAt":null,"executionTime":33474,"lastExecutedAt":1719850563887,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Start coding here\n# Before you start\n# In order to complete the project you may wish to install SpeechRecognition, Pydub and spaCy libraries and download pretrained spaCy small English Language model.\n\n!pip install SpeechRecognition\n!pip install pydub\n!pip install spacy\n!python3 -m spacy download en_core_web_sm\n\n# Import required libraries\nimport pandas as pd\n\nimport nltk\nnltk.download('vader_lexicon')\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer\n\nimport speech_recognition as sr\nfrom pydub import AudioSegment\n\nimport spacy\n\n# Task 1 - Speech to Text: convert the sample audio call, sample_customer_call.wav, to text and store the result in transcribed_text\n\n# Define a recognizer object\nrecognizer = sr.Recognizer()\n\n# Convert the audio file to audio data\ntranscribe_audio_file = sr.AudioFile(\"sample_customer_call.wav\")\nwith transcribe_audio_file as source:\n    transcribe_audio = recognizer.record(source)\n\n# Convert the audio data to text\ntranscribed_text = recognizer.recognize_google(transcribe_audio)\n\n# Review trascribed text\nprint(\"Transcribed text: \", transcribed_text)\n\n# Task 1 - Speech to Text: store few statistics of the audio file such as number of channels, sample width and frame rate\n    \n# Review number of channels and frame rate of the audio file\naudio_segment = AudioSegment.from_file(\"sample_customer_call.wav\")\nnumber_channels = audio_segment.channels\nframe_rate = audio_segment.frame_rate\n\nprint(\"Number of channels: \", number_channels)\nprint(\"Frame rate: \", frame_rate)\n\n# Task 2 - Sentiment Analysis: use vader module from nltk library to determine the sentiment of each text of the customer_call_transcriptions.csv file and store them at a new sentiment_label column using compound score\n\n# Import customer call transcriptions data\ndf = pd.read_csv(\"customer_call_transcriptions.csv\")\n\nsid = SentimentIntensityAnalyzer()\n\n# Analyze sentiment by evaluating compound score generated by Vader SentimentIntensityAnalyzer\ndef find_sentiment(text):\n    scores = sid.polarity_scores(text)\n    compound_score = scores['compound']\n\n    if compound_score >= 0.05:\n        return 'positive'\n    elif compound_score <= -0.05:\n        return 'negative'\n    else:\n        return 'neutral'\n\ndf['sentiment_predicted'] = df.apply(lambda row: find_sentiment(row[\"text\"]), axis = 1)\n\n# Task 2 - Sentiment Analysis: calculate number of texts with positive label that are correctly labeled as positive\ntrue_positive = len(df.loc[(df['sentiment_predicted'] == df['sentiment_label']) &\n                (df['sentiment_label'] == 'positive')])\n\nprint(\"True positives: \", true_positive)\n\n# Task 3 - Named Entity Recognition: find named entities for each text in the df object and store entities in a named_entities column\n\n# Load spaCy small English Language model\nnlp = spacy.load(\"en_core_web_sm\")\n\n# NER using spaCy\ndef extract_entities(text):\n    doc = nlp(text)\n    entities = [ent.text for ent in doc.ents]\n    return entities\n\n# Apply NER to the entire text column\ndf['named_entities'] = df['text'].apply(extract_entities)\n\n# Flatten the list of named entities\nall_entities = [ent for entities in df['named_entities'] for ent in entities]\n\n# Create a DataFrame with the counts\nentities_df = pd.DataFrame(all_entities, columns=['entity'])\nentities_counts = entities_df['entity'].value_counts().reset_index()\nentities_counts.columns = ['entity', 'count']\n\n# Extract most frequent named entity\nmost_freq_ent = entities_counts[\"entity\"].iloc[0]\nprint(\"Most frequent entity: \", most_freq_ent)\n\n# Task 4 - Find most similar text: find the list of customer calls that complained about \"wrong package delivery\" by finding similarity score of each text to the \"wrong package delivery\" string using spaCy small English Language model\n\n# Load spaCy model\nnlp = spacy.load(\"en_core_web_sm\")\n\n# Process the text column\ndf['processed_text'] = df['text'].apply(lambda text: nlp(text))\n\n# Input query\ninput_query = \"wrong package delivery\"\nprocessed_query = nlp(input_query)\n\n# Calculate similarity scores and sort dataframe with respect to similarity scores\ndf['similarity'] = df['processed_text'].apply(lambda text: processed_query.similarity(text))\ndf = df.sort_values(by='similarity', ascending=False)\n\n# Find the most similar text\nmost_similar_text = df[\"text\"].iloc[0]\nprint(\"Most similar text: \", most_similar_text)","outputsMetadata":{"0":{"height":412,"type":"stream"},"1":{"height":80,"type":"stream"},"2":{"height":143,"type":"stream"}},"lastExecutedByKernel":"a92e50f9-461c-4d7f-8096-a48d24039401","colab":{"base_uri":"https://localhost:8080/"},"id":"250524c2-1bd3-4ff8-a224-8fa007566c1b","executionInfo":{"status":"ok","timestamp":1723966067727,"user_tz":-300,"elapsed":6018,"user":{"displayName":"Saad R","userId":"07513358746274001782"}},"outputId":"1212b455-5f51-498a-ade6-18fb5bd5d575"},"id":"250524c2-1bd3-4ff8-a224-8fa007566c1b","cell_type":"code","execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["Transcribed text:  hello I'm experiencing an issue with your product I'd like to speak to someone about a replacement\n","Number of channels:  1\n","Frequency:  44100\n","True positives:  2\n","Most frequent entity:  yesterday\n","Most similar text:  wrong package delivered\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-7-fcda1e286217>:93: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n","  df['similarity'] = df['processed_text'].apply(lambda text: processed_query.similarity(text))\n"]}]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.10"}},"nbformat":4,"nbformat_minor":5}